{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanpang028/hello-world/blob/master/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEw5euHO1sw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wTxs2S_1t3i",
        "colab_type": "text"
      },
      "source": [
        "# Generative Adversarial Nets\n",
        "## Introduction\n",
        "### Content\n",
        "GAN's core idea comes from the Nash equilibrium of game theory. It sets the players as an Generator and a Discriminator, respectively. The purpose of the Generator is to learn as much as possible about the real data distribution, while the purpose of the Discriminator is to determine as correctly as possible whether the input data comes from the real data or the Generator. In order to win the game, the two players need to constantly improve their ability to generate and distinguish, and this learning optimization process is to find a Nash equilibrium between them. This paper proposes a new framework to evaluate the generative model through a confrontation process in which we simultaneously train two models: a generative model and a discrimination model. G captures data distribution, the discrimination model estimates the probability samples from the training data rather than generating model. In this paper, the author discusses the special case in which the generation model transmits random noise to generate samples through multilayer perceptron. The author calls this special case adversarial net. In this case, it can train both models using only the very successful back propagation and deletion algorithms, and use only forward propagation to extract samples from the generated model. There is no need for approximate reasoning or markov chains.In this paper, the task is to study a special case where a sample can be generated from the generated model G by injecting a segment of noise into the model. Both the generation model G and the discriminant model are multi-layer perceptual structures. The authors refer to this situation as the adversarial network. In this framework, both models can be trained using backward propagation and dropout.\n",
        "\n",
        "### Innovation¶\n",
        "Disadvantages of generating adversarial network: Training GAN needs to reach Nash equilibrium, sometimes it can be achieved by gradient descent, sometimes it can not. We have not found a good way to achieve Nash equilibrium, so training GAN is not stable compared with VAE or PixelRNN, but I think it is more stable than training boltzmann machine in practice GAN is not suitable for processing data in discrete form, such as text GAN has problems of unstable training, gradient disappearance and mode collapse After understanding the generation model and the discriminant model, it is very straightforward to understand the adversarial network. Adversarial network only proposes a network structure. The innovation of this paper lies in a new model -- generative antagonism model. This model is actually training two contra-fighting networks, one for an Generator and the other for a Descriminator. The two networks are pitted against each other, and the generator's goal is to produce as close as possible to the real picture as possible, and the measure is the discriminator, and the generator has to make it impossible for the discriminator to tell whether an image is a distribution from the real image or a distribution learned by the generator.In general, the whole framework is very simple. GANs' simple idea is to use two models, one to generate the model and one to test the model. The model is used to determine whether a given image is a real image (an image taken from a data set). The task of the model is to create an image that looks like a real image. And in the beginning the two models are not trained, these two models together confrontation training, discriminant model generation model to produce a picture to deceive, then discriminant model to judge the picture is true or not, eventually in the process of these two model training, the ability of two models is more and more strong, eventually reach steady state.\n",
        "\n",
        "### Technical quality\n",
        "Theoretical and experimental results are presented in this paper.They trained adversarial nets using an a range of datasets including MNIST, the Toronto Face Database (TFD), and CIFAR-10. The generator uses a mixture of ReLU and Sigmoid activation units, while the discriminator uses maxout activation units and uses Dropout for network training. Although the theoretical framework can be used for Dropout and other noise at the middle level of the generator, here noise input is used only at the bottom level of the generation network. The main technical advantage of this article is computing. The antagonism model also gains some statistical advantages from the generation network, not by updating the data samples directly, but by the gradient that flows through the discriminator. This means that the input component is not copied directly to the generator's parameters. Another advantage of adversarial networks is that they can represent very clear and even degenerate distributions, whereas markov chain-based methods require a somewhat fuzzy distribution so that chains can be mixed between patterns.GANs is relatively new and needs further study. In particular, GANs requires Nash equilibrium in high-dimensional, continuous, non-convex strategies. Researchers should strive to develop better theoretical basis and training algorithm. GANs is of great significance for image generation, manipulation system and many other aspects, and may be applied to a wider range of fields in the future.\n",
        "\n",
        "### Application and X-factor\n",
        "This paper demonstrates the feasibility of the generative adversarial nets and indicates that these research directions may be useful.The pictures generated by the experiment are not better than those generated by other algorithms, but it can be seen that GAN has certain potential.Through a large number of experiments, the author compares the present example-based evaluation methods.This research can be applied to image processing, natural language processing and integration with reinforcement learning.Through reading this article, I feel that the idea of \"Game Theory\" has great application value in real life.\n",
        "\n",
        "### Presentation\n",
        "I think this paper is worth learning from its style of discussion, as well as its writing style.This paper has some originality and innovation, put forward their own algorithm and a lot of experiments to prove the correctness of their algorithm.\n",
        "\n",
        "### References\n",
        "[1] Bengio, Y. 2016.”InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets”\n",
        "\n",
        "[2] Yu L, Zhang W, Wang J, et al. 2016. ”SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient”. \n",
        "\n",
        "[3] Li C, Xu K, Zhu J, et al. 2017.”Triple Generative Adversarial Nets”. \n",
        "\n",
        "[4] Yang Z, Chen W, Wang F, et al. 2017.”Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets”.\n"
      ]
    }
  ]
}