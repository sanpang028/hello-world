{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanpang028/hello-world/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX9yOvyY9h_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNANGDP3-B9K",
        "colab_type": "text"
      },
      "source": [
        "# Generative Adversarial Nets\n",
        "## Introduction\n",
        "### Content\n",
        "As a deep learning model, generative adversarial network is one of the most promising methods for unsupervised learning in complex distribution in recent years. The model produces a fairly good output through the mutual game learning of two modules in the framework: generating model and discriminating model. In the original GAN theory, both G and D are not required to be neural networks, only the functions generated and judged can be fitted. But in practice, deep neural networks are generally used as G and D. This paper proposes a new framework to evaluate the generative model through a confrontation process in which we simultaneously train two models: a generative model G capture data distribution,D and the discrimination model estimate the probability samples from the training data rather than G. The training process is D make a mistake to maximize the probability. In this paper, the author discusses the special case in which the generation model transmits random noise to generate samples through multilayer perceptron. The author calls this special case adversarial net. In this case, it can train both models using only the very successful back propagation and deletion algorithms, and use only forward propagation to extract samples from the generated model. There is no need for approximate reasoning or markov chains.In this paper, the task is to study a special case where a sample can be generated from the generated model G by injecting a segment of noise into the model. Both the generation model G and the discriminant model are multi-layer perceptual structures. The authors refer to this situation as the adversarial network. In this framework, both models can be trained using backward propagation and dropout.\n",
        "\n",
        "### Innovation\n",
        "Disadvantages of generating adversarial network: Training GAN needs to reach Nash equilibrium, sometimes it can be achieved by gradient descent, sometimes it can not. We have not found a good way to achieve Nash equilibrium, so training GAN is not stable compared with VAE or PixelRNN, but I think it is more stable than training boltzmann machine in practice GAN is not suitable for processing data in discrete form, such as text GAN has problems of unstable training, gradient disappearance and mode collapse The paper proposes a new generative model estimation procedure that sidesteps these difficulties. In the proposed adversarial nets framework, the generative model is pitted against an adversary: a discriminative model that learns to determine whether a sample is from the model distribution or the data distribution. The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.After understanding the generation model and the discriminant model, it is very straightforward to understand the adversarial network. Adversarial network only proposes a network structure. In general, the whole framework is very simple. GANs' simple idea is to use two models, one to generate the model and one to test the model. The model is used to determine whether a given image is a real image (an image taken from a data set). The task of the model is to create an image that looks like a real image. And in the beginning the two models are not trained, these two models together confrontation training, discriminant model generation model to produce a picture to deceive, then discriminant model to judge the picture is true or not, eventually in the process of these two model training, the ability of two models is more and more strong, eventually reach steady state.\n",
        "\n",
        "### Technical quality\n",
        "Theoretical and experimental results are presented in this paper.They trained adversarial nets an a range of datasets including MNIST, the Toronto Face Database (TFD), and CIFAR-10. Trained antagonistic generation networks on datasets MNIST, Toronto Face Database and cifar-10. The generator uses a mixture of ReLU and Sigmoid activation units, while the discriminator uses maxout activation units and uses Dropout for network training. Although the theoretical framework can be used for Dropout and other noise at the middle level of the generator, here noise input is used only at the bottom level of the generation network. While this paper makes no claim that these samples are better than samples generated by existing methods, we believe that these samples are at least competitive with the better generative models in the literature and highlight the potential of the adversarial framework.The advantages is that only backprop is used to obtain gradients, no inference is needed during learning, and a wide variety of functions can be incorporated into the model.GANs is relatively new and needs further study. In particular, GANs requires Nash equilibrium in high-dimensional, continuous, non-convex strategies. Researchers should strive to develop better theoretical basis and training algorithm. GANs is of great significance for image generation, manipulation system and many other aspects, and may be applied to a wider range of fields in the future.\n",
        "\n",
        "### Application and X-factor\n",
        "This paper has demonstrated the viability of the adversarial modeling framework, suggesting that these research directions could prove useful.The pictures generated by the experiment are not better than those generated by other algorithms, but it can be seen that GAN has certain potential.Through a large number of experiments, the author compares the present example-based evaluation methods.This research can be applied to image processing, natural language processing and integration with reinforcement learning.Through reading this article, I feel that the idea of \"Game Theory\" has great application value in real life.\n",
        "\n",
        "Presentation\n",
        "I think this paper is worth learning from its style of discussion, as well as its writing style.This paper has some originality and innovation, put forward their own algorithm and a lot of experiments to prove the correctness of their algorithm.\n",
        "\n",
        "References\n",
        "\n",
        "[1] Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.\n",
        "\n",
        "[2] Bengio, Y. (2009). Learning deep architectures for AI. Now Publishers. \n",
        "\n",
        "[3] Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013a). Better mixing via deep representations. In ICML’13. \n",
        "\n",
        "[4] Bengio, Y., Yao, L., Alain, G., and Vincent, P. (2013b). Generalized denoising auto-encoders as generative models. In NIPS26. Nips Foundation. \n",
        "\n",
        "[5] Bengio, Y., Thibodeau-Laufer, E., and Yosinski, J. (2014a). Deep generative stochastic networks trainable by backprop. In ICML’14."
      ]
    }
  ]
}